{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad9de032",
   "metadata": {},
   "source": [
    "**Error Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d28731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import earthaccess\n",
    "import xarray as xr\n",
    "import dask\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import cartopy.feature as cfeature\n",
    "from rasterio import features\n",
    "from scipy.ndimage import convolve\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.transform import from_origin\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# colormap for plotting sea ice throughout rest of project\n",
    "\n",
    "cmap = plt.get_cmap(\"Blues\").copy()\n",
    "cmap.set_bad(color='lightgray')\n",
    "\n",
    "# authenticate NASA earth access\n",
    "\n",
    "auth = earthaccess.login(strategy='interactive', persist = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fdbe83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we found 13204 results\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataGranule' object has no attribute 'href'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwe found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m results\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# open files in earthaccess\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m urls = [\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhref\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33ms3\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m r.href(\u001b[33m\"\u001b[39m\u001b[33mhttps\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "\u001b[31mAttributeError\u001b[39m: 'DataGranule' object has no attribute 'href'"
     ]
    }
   ],
   "source": [
    "# search NASA database\n",
    "\n",
    "results = earthaccess.search_data(\n",
    "    short_name='NSIDC-0051',\n",
    "    temporal=('1990-01-01', '2025-10-01'),\n",
    "    bounding_box=(-180, 0, 180, 90),\n",
    "    cloud_hosted=True\n",
    ")\n",
    "\n",
    "print(f\"we found {len(results)} results\")\n",
    "\n",
    "# open files in earthaccess\n",
    "\n",
    "files = earthaccess.open(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005945f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Unable to synchronously open attribute (invalid identifier type to function)'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# open data in xarray\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ds = \u001b[43mxr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mh5netcdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcombine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mby_coords\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/coastal-ice/lib/python3.12/site-packages/xarray/backends/api.py:1643\u001b[39m, in \u001b[36mopen_mfdataset\u001b[39m\u001b[34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[39m\n\u001b[32m   1638\u001b[39m     datasets = [preprocess(ds) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[32m   1640\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m parallel:\n\u001b[32m   1641\u001b[39m     \u001b[38;5;66;03m# calling compute here will return the datasets/file_objs lists,\u001b[39;00m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;66;03m# the underlying datasets will still be stored as dask arrays\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1643\u001b[39m     datasets, closers = \u001b[43mdask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1645\u001b[39m \u001b[38;5;66;03m# Combine all datasets, closing them in case of a ValueError\u001b[39;00m\n\u001b[32m   1646\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/coastal-ice/lib/python3.12/site-packages/dask/base.py:681\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     expr = expr.optimize()\n\u001b[32m    679\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(flatten(expr.__dask_keys__()))\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/coastal-ice/lib/python3.12/site-packages/xarray/backends/api.py:687\u001b[39m, in \u001b[36mopen_dataset\u001b[39m\u001b[34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[39m\n\u001b[32m    675\u001b[39m decoders = _resolve_decoders_kwargs(\n\u001b[32m    676\u001b[39m     decode_cf,\n\u001b[32m    677\u001b[39m     open_backend_dataset_parameters=backend.open_dataset_parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    683\u001b[39m     decode_coords=decode_coords,\n\u001b[32m    684\u001b[39m )\n\u001b[32m    686\u001b[39m overwrite_encoded_chunks = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33moverwrite_encoded_chunks\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m backend_ds = \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    693\u001b[39m ds = _dataset_from_backend_dataset(\n\u001b[32m    694\u001b[39m     backend_ds,\n\u001b[32m    695\u001b[39m     filename_or_obj,\n\u001b[32m   (...)\u001b[39m\u001b[32m    705\u001b[39m     **kwargs,\n\u001b[32m    706\u001b[39m )\n\u001b[32m    707\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/coastal-ice/lib/python3.12/site-packages/xarray/backends/h5netcdf_.py:473\u001b[39m, in \u001b[36mH5netcdfBackendEntrypoint.open_dataset\u001b[39m\u001b[34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, format, group, lock, invalid_netcdf, phony_dims, decode_vlen_strings, driver, driver_kwds, storage_options)\u001b[39m\n\u001b[32m    458\u001b[39m store = H5NetCDFStore.open(\n\u001b[32m    459\u001b[39m     filename_or_obj,\n\u001b[32m    460\u001b[39m     \u001b[38;5;28mformat\u001b[39m=\u001b[38;5;28mformat\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    468\u001b[39m     storage_options=storage_options,\n\u001b[32m    469\u001b[39m )\n\u001b[32m    471\u001b[39m store_entrypoint = StoreBackendEntrypoint()\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m ds = \u001b[43mstore_entrypoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_coords\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# only warn if phony_dims exist in file\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# remove together with the above check\u001b[39;00m\n\u001b[32m    486\u001b[39m \u001b[38;5;66;03m# after some versions\u001b[39;00m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m store.ds._root._phony_dim_count > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m emit_phony_dims_warning:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/coastal-ice/lib/python3.12/site-packages/xarray/backends/store.py:44\u001b[39m, in \u001b[36mStoreBackendEntrypoint.open_dataset\u001b[39m\u001b[34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen_dataset\u001b[39m(\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     32\u001b[39m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m | os.PathLike[Any] | ReadBuffer | AbstractDataStore,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     decode_timedelta=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     41\u001b[39m ) -> Dataset:\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename_or_obj, AbstractDataStore)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28mvars\u001b[39m, attrs = \u001b[43mfilename_or_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     encoding = filename_or_obj.get_encoding()\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mvars\u001b[39m, attrs, coord_names = conventions.decode_cf_variables(\n\u001b[32m     48\u001b[39m         \u001b[38;5;28mvars\u001b[39m,\n\u001b[32m     49\u001b[39m         attrs,\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m         decode_timedelta=decode_timedelta,\n\u001b[32m     57\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/coastal-ice/lib/python3.12/site-packages/xarray/backends/common.py:314\u001b[39m, in \u001b[36mAbstractDataStore.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    295\u001b[39m \u001b[33;03mThis loads the variables and attributes simultaneously.\u001b[39;00m\n\u001b[32m    296\u001b[39m \u001b[33;03mA centralized loading function makes it easier to create\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m \u001b[33;03mare requested, so care should be taken to make sure its fast.\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    311\u001b[39m variables = FrozenDict(\n\u001b[32m    312\u001b[39m     (_decode_variable_name(k), v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_variables().items()\n\u001b[32m    313\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m attributes = FrozenDict(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_attrs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m variables, attributes\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/coastal-ice/lib/python3.12/site-packages/xarray/backends/h5netcdf_.py:266\u001b[39m, in \u001b[36mH5NetCDFStore.get_attrs\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_attrs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m FrozenDict(\u001b[43m_read_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/coastal-ice/lib/python3.12/site-packages/xarray/backends/h5netcdf_.py:70\u001b[39m, in \u001b[36m_read_attributes\u001b[39m\u001b[34m(h5netcdf_var)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_attributes\u001b[39m(h5netcdf_var):\n\u001b[32m     66\u001b[39m     \u001b[38;5;66;03m# GH451\u001b[39;00m\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# to ensure conventions decoding works properly on Python 3, decode all\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# bytes attributes to strings\u001b[39;00m\n\u001b[32m     69\u001b[39m     attrs = {}\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mh5netcdf_var\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_FillValue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmissing_value\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen _collections_abc>:894\u001b[39m, in \u001b[36m__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/coastal-ice/lib/python3.12/site-packages/h5netcdf/attrs.py:32\u001b[39m, in \u001b[36mAttributes.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# get original attribute via h5py low level api\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# see https://github.com/h5py/h5py/issues/2045\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h5py.\u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mh5py\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     attr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_h5attrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     34\u001b[39m     attr = \u001b[38;5;28mself\u001b[39m._h5attrs[key]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/coastal-ice/lib/python3.12/site-packages/h5py/_hl/attrs.py:94\u001b[39m, in \u001b[36mAttributeManager.get_id\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_id\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[32m     92\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a low-level AttrID object for the named attribute.\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mh5a\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:56\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:57\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5a.pyx:80\u001b[39m, in \u001b[36mh5py.h5a.open\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Unable to synchronously open attribute (invalid identifier type to function)'"
     ]
    }
   ],
   "source": [
    "# open data in xarray\n",
    "\n",
    "ds = xr.open_mfdataset(\n",
    "    urls, \n",
    "    engine=\"h5netcdf\",\n",
    "    parallel=True, \n",
    "    combine='by_coords',\n",
    "    chunks={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fa33a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in land file from geopandas\n",
    "\n",
    "land = gpd.read_file(\"../data/ne_10m_land/ne_10m_land.shp\")\n",
    "land = land.to_crs(epsg=3411)\n",
    "\n",
    "# create affine transform and make sure arctic is not upside down\n",
    "\n",
    "dx = float(ds.x.diff('x').mean())\n",
    "dy = float(ds.y.diff('y').mean())\n",
    "x0 = float(ds.x.min())\n",
    "y0 = float(ds.y.max())\n",
    "\n",
    "transform = [dx, 0, x0, 0, -abs(dy), y0]\n",
    "\n",
    "# use transform to mask out coastal cells\n",
    "\n",
    "land_mask = features.rasterize(\n",
    "    ((geom, 1) for geom in land.geometry),\n",
    "    out_shape=(ds.sizes['y'], ds.sizes['x']),\n",
    "    transform=transform,\n",
    "    fill=0,\n",
    "    dtype=np.uint8\n",
    ")\n",
    "\n",
    "# calculate distance from land using euclidian distance transform\n",
    "\n",
    "distance_from_land = distance_transform_edt(land_mask == 0)\n",
    "\n",
    "# convert to xarray.DataArray\n",
    "\n",
    "distance_xr = xr.DataArray(\n",
    "    distance_from_land,\n",
    "    coords={'y': ds.y, 'x': ds.x},\n",
    "    dims=('y', 'x'),\n",
    "    name='distance_to_land_cells'\n",
    ")\n",
    "\n",
    "# add as data variable in ds\n",
    "\n",
    "ds['edtl'] = distance_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in files\n",
    "\n",
    "folderpath = 'scratch/fld1/visual_ice/'\n",
    "pathlist = Path(folderpath).glob(\"*.csv\")\n",
    "\n",
    "# convert rows and columns into lats and lons\n",
    "\n",
    "row_to_lat = dict(enumerate(ds['x'].values))\n",
    "col_to_lon = dict(enumerate(ds['y'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb87c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through files\n",
    "\n",
    "for i, file in enumerate(pathlist):\n",
    "\n",
    "    # read in single file and map to lat / lon\n",
    "\n",
    "    visual = pd.read_csv(str(file))\n",
    "    visual[\"time\"] = pd.to_datetime(visual[\"Date\"], yearfirst=True)\n",
    "    visual['x'] = visual['Row'].map(row_to_lat)\n",
    "    visual['y'] = visual['Column'].map(col_to_lon)\n",
    "\n",
    "    # convert to xarray\n",
    "\n",
    "    da_sparse = visual.set_index(['time', 'y', 'x']).to_xarray()\n",
    "    da_full = da_sparse.reindex_like(ds, method=None).chunk({'time': 2})\n",
    "\n",
    "    # concatinate into the main dataset (or create new data variable for first file)\n",
    "\n",
    "    if i == 0:\n",
    "        ds['visual_ice'] = da_full['SI frac']\n",
    "    else:\n",
    "        ds['visual_ice'] = xr.concat([ds['SI_frac'], da_full['SI frac']], dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc318604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "\n",
    "condition = ((ds.visual_ice.notnull()) & (ds.F17_ICECON < 1.0))\n",
    "ds_clean = ds.where(condition, other=np.nan).compute()\n",
    "\n",
    "df = ds_clean.to_dataframe().reset_index().dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate error\n",
    "\n",
    "error = ((ds_clean['F17_ICECON'] - ds_clean['visual_ice']))\n",
    "\n",
    "# mse\n",
    "mse = ((((ds_clean['F17_ICECON'] - ds_clean['visual_ice']))**2)**0.5)\n",
    "avg_mse = mse.mean(dim=['time', 'x', 'y'], skipna=True)\n",
    "print('Simple error is', avg_mse.compute().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb8d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependancies\n",
    "#   – sort dataset see if there are differences (how much land, weather, snow cover, clouds, sun elevation)\n",
    "#   – look at date but not enough dates in sample data\n",
    "#   – regression, plotting, divide into populations, light gradient boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9591d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean that table\n",
    "# there are lots of 1.012 and 1.016 which is obviously impossible but look in NSIDC docs it means something\n",
    "# calculate error with a sign"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coastal-ice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
